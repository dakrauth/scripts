#!/usr/bin/env python
import re
import sys
import time
import logging
import textwrap
from pprint import pprint
from decimal import Decimal
from datetime import datetime, timedelta

import bleach
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger('wiki')


ODD_ERROR_MESSAGE = "This shouldn't happen. Please report on GitHub: github.com/goldsmith/Wikipedia"


class WikipediaException(Exception):
    pass


class PageError(WikipediaException):
    """Exception raised when no Wikipedia matched a query."""

    def __str__(self):
        return f'{self.args[0]} "{self.args[1]}" does not match any pages. Try another query!'


class DisambiguationError(WikipediaException):
    """Exception raised when a page resolves to a Disambiguation page."""

    def __str__(self):
        return f'"{self.args[0]}" may refer to: {self.args[1]}'


class RedirectError(WikipediaException):
    """Exception raised when a page title unexpectedly resolves to a redirect."""

    def __str__(self):
        return f'"{self.args[0]}" redirected. Set redirect = True to allow redirects.'


class HTTPTimeoutError(WikipediaException):
    """Exception raised when a request to the Mediawiki servers times out."""

    def __str__(self):
        return (
            f'Timeout searching "{self.args[0]}". '
            'Try again in a few seconds, and make sure you have rate limiting set to True.'
        )


def page_key(value):
    if isinstance(value, str) and value.isdigit():
        value = int(value)

    if isinstance(value, str):
        return ('title', value)

    if isinstance(value, int):
        return ('pageid', value)

    raise ValueError('value must be either str or int')


class WikipediaPage:
    '''
    Contains data from a Wikipedia page.
    '''

    preload_opts = ('content', 'summary', 'sections', 'categories')

    def __init__(
        self,
        title,
        wiki=None,
        redirect=True,
        preload=False
    ):
        self.wiki = wiki or Wikipedia()
        self.title = self.pageid = None
        self.key = (kind, value) = page_key(title)
        self.data = {}

        if kind == 'title':
            self.title = value
        else:
            self.pageid = value

        #import ipdb; ipdb.set_trace()
        self.load(redirect=redirect, preload=preload)
        if preload:
            preload = self.preload_opts if preload is True else preload
            for prop in preload:
                getattr(self, prop)

    def write(self, stream=None):
        import os
        width = os.get_terminal_size().columns
        stream = stream or sys.stdout
        print(self.title, file=stream, end='\n\n')
        for line in self.summary.splitlines():
            for wrapped in textwrap.wrap(line, width=width):
                print(wrapped, file=stream)
            print()

    def __repr__(self):
        return '<WikipediaPage "{}">'.format(self.title or self.pageid)

    def __eq__(self, other):
        return (
            self.pageid == other.pageid
            and self.title == other.title
            and self.url == other.url
        )

    def load(self, redirect=True, preload=()):
        '''
        Load basic information from Wikipedia.
        Confirm that page exists and is not a disambiguation/redirect.

        Does not need to be called manually, should be called automatically during __init__.
        '''
        name, value = self.key
        query_params = {
            'prop': 'info|pageprops',
            'inprop': 'url',
            'ppprop': 'disambiguation',
            'redirects': '',
        }
        query_params[f'{name}s'] = value

        request = self.wiki.request(**query_params)

        query = request['query']
        pageid, page = list(query['pages'].items())[0]

        # missing is present if the page is missing
        if 'missing' in page:
            raise PageError(name, value)

        # same thing for redirect, except it shows up in query instead of page for
        # whatever silly reason
        elif 'redirects' in query:
            if redirect:
                redirects = query['redirects'][0]

                if 'normalized' in query:
                    normalized = query['normalized'][0]
                    assert normalized['from'] == self.title, ODD_ERROR_MESSAGE

                    from_title = normalized['to']

                else:
                    from_title = self.title

                assert redirects['from'] == from_title, ODD_ERROR_MESSAGE

                # change the title and reload the whole object
                self.__init__(redirects['to'], redirect=redirect, preload=preload)

            else:
                raise RedirectError(getattr(self, 'title', page['title']))

        # since we only asked for disambiguation in ppprop,
        # if a pageprop is returned,
        # then the page must be a disambiguation page
        elif 'pageprops' in page:
            query_params = {
                'prop': 'revisions',
                'rvprop': 'content',
                'rvparse': '',
                'rvlimit': 1
            }
            if self.pageid:
                query_params['pageids'] = self.pageid
            else:
                query_params['titles'] = self.title

            request = self.wiki.request(**query_params)
            html = request['query']['pages'][pageid]['revisions'][0]['*']

            lis = BeautifulSoup(html, 'html.parser').find_all('li')
            filtered_lis = [li for li in lis if not 'tocsection' in ''.join(li.get('class', []))]
            may_refer_to = [li.a.get_text() for li in filtered_lis if li.a]

            raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)

        else:
            self.pageid = pageid
            self.title = page['title']
            self.url = page['fullurl']
            self.data.update(page)

    def __continued_query(self, query_params):
        '''
        Based on https://www.mediawiki.org/wiki/API:Query#Continuing_queries
        '''
        query_params.update({'titles': self.title} if self.title else {'pageids': self.pageid})

        last_continue = {}
        prop = query_params.get('prop', None)

        while True:
            params = query_params.copy()
            params.update(last_continue)

            request = self.wiki.request(**params)

            if 'query' not in request:
                break

            pages = request['query']['pages']
            yield from pages.values()

            if 'continue' not in request:
                break

            last_continue = request['continue']

    def html(self):
        '''
        Get full page HTML.

        .. warning:: This can get pretty slow on long pages.
        '''

        if 'html' not in self.data:
            request = self.wiki.request(
                prop='revisions',
                rvprop='content',
                rvlimit=1,
                rvparse='',
                titles=self.title
            )
            self.data['html'] = request['query']['pages'][self.pageid]['revisions'][0]['*']

        return self.data['html']

    @property
    def content(self):
        '''
        Plain text content of the page, excluding images, tables, and other data.
        '''

        if 'content' not in self.data:
            query_params = {
                'prop': 'extracts|revisions',
                'explaintext': '',
                'rvprop': 'ids'
            }
            if self.title:
                  query_params['titles'] = self.title
            else:
                  query_params['pageids'] = self.pageid

            request = self.wiki.request(**query_params)
            pages = request['query']['pages']
            self.data['content'] = pages[self.pageid]['extract']
            self.data['revision_id'] = pages[self.pageid]['revisions'][0]['revid']
            self.data['parent_id'] = pages[self.pageid]['revisions'][0]['parentid']

        return self.data['content']

    @property
    def revision_id(self):
        '''
        Revision ID of the page.

        The revision ID is a number that uniquely identifies the current
        version of the page. It can be used to create the permalink or for
        other direct API calls. See `Help:Page history
        <http://en.wikipedia.org/wiki/Wikipedia:Revision>`_ for more
        information.
        '''

        if 'revision_id' not in self.data:
            # fetch the content (side effect is loading the revid)
            self.content

        return self.data['revision_id']

    @property
    def parent_id(self):
        '''
        Revision ID of the parent version of the current revision of this
        page. See ``revision_id`` for more information.
        '''

        if 'parentid' not in self.content:
            # fetch the content (side effect is loading the revid)
            self.content

        return self.data['parent_id']

    @property
    def summary(self):
        '''
        Plain text summary of the page.
        '''

        if 'summary' not in self.data:
            query_params = {'prop': 'extracts', 'explaintext': '', 'exintro': ''}
            if not getattr(self, 'title', None) is None:
                  query_params['titles'] = self.title
            else:
                  query_params['pageids'] = self.pageid

            request = self.wiki.request(**query_params)
            self.data['summary'] = request['query']['pages'][self.pageid]['extract']

        return self.data['summary']

    @property
    def coordinates(self):
        '''
        Tuple of Decimals in the form of (lat, lon) or None
        '''
        if 'coordinates' not in self.data:
            request = self.wiki.request(
                prop='coordinates',
                colimit='max',
                titles=self.title,
            )

            if 'query' in request:
                coordinates = request['query']['pages'][self.pageid]['coordinates']
                self.data['coordinates'] = (Decimal(coordinates[0]['lat']), Decimal(coordinates[0]['lon']))
            else:
                self.data['coordinates'] = None

        return self.data['coordinates']

    @property
    def categories(self):
        '''
        List of categories of a page.
        '''

        if 'categories' not in self.data:
            self.data['categories'] = [re.sub(r'^Category:', '', x) for x in
                [link['title'] for link in self.__continued_query({
                    'prop': 'categories',
                    'cllimit': 'max'
                })
            ]]

        return self.data['categories']

    @property
    def sections(self):
        '''
        List of section titles from the table of contents on the page.
        '''

        if 'sections' not in self.data:
            query_params = {
                'action': 'parse',
                'prop': 'sections',
            }
            if not getattr(self, 'title', None) is None:
                query_params["page"] = self.title

            request = self.wiki.request(**query_params)
            self.data['sections'] = [section['line'] for section in request['parse']['sections']]

        return self.data['sections']

    def section(self, section_title):
        '''
        Get the plain text content of a section from `self.sections`.
        Returns None if `section_title` isn't found, otherwise returns a whitespace stripped string.

        This is a convenience method that wraps self.content.

        .. warning:: Calling `section` on a section that has subheadings will NOT return
                      the full text of all of the subsections. It only gets the text between
                      `section_title` and the next subheading, which is often empty.
        '''

        section = "== {} ==".format(section_title)
        try:
            index = self.content.index(section) + len(section)
        except ValueError:
            return None

        try:
            next_index = self.content.index("==", index)
        except ValueError:
            next_index = len(self.content)

        return self.content[index:next_index].lstrip("=").strip()


class Wikipedia:
    api_url = 'http://en.wikipedia.org/w/api.php'
    rate_limit = False
    rate_limit_min_wait = None
    user_agent = 'wikipedia console v1.0.0'

    def __init__(
        self,
        language_prefix=None,
        user_agent=None,
        rate_limit=None,
        rate_limit_min_wait=None
    ):
        self.rate_limit_last_call = None
        if language_prefix:
            # Set `prefix` to one of the two letter prefixes found on the
            # list of all Wikipedias <http://meta.wikimedia.org/wiki/List_of_Wikipedias>
            self.api_url = 'http://' + language_prefix.lower() + '.wikipedia.org/w/api.php'

        if user_agent:
            self.user_agent = user_agent_string

        if rate_limit:
            self.rate_limit = rate_limit
            self.rate_limit_min_wait = rate_limit_min_wait

    def request(self, **params):
        '''
        Make a request to the Wikipedia API using the given search parameters.
        Returns a parsed dict of the JSON response.
        '''

        params.setdefault('format', 'json')
        params.setdefault('action', 'query')
        if(
            self.rate_limit and
            self.rate_limit_last_call and
            self.rate_limit_last_call + self.rate_limit_min_wait > datetime.now()
        ):
            # it hasn't been long enough since the last API call
            # so wait until we're in the clear to make the request
            wait_time = (self.rate_limit_last_call + self.rate_limit_min_wait) - datetime.now()
            time.sleep(int(wait_time.total_seconds()))

        r = requests.get(self.api_url, params=params, headers={'User-Agent': self.user_agent})

        if self.rate_limit:
            self.rate_limit_last_call = datetime.now()

        return r.json()

    def page(self, title, auto_suggest=10, redirect=True, preload=False):
        '''
        Get a WikipediaPage object for the page. `title` can be a string title or the
        int pageid.

        Keyword arguments:

        * auto_suggest - let Wikipedia find a valid page title for the query
        * redirect - allow redirection without raising RedirectError
        * preload - load content, summary, images, references, and links during initialization
        '''
        if auto_suggest:
            results, suggestion = self.search(title, results=auto_suggest, suggestion=True)
            try:
                title = suggestion or results[0]
            except IndexError:
                # if there is no suggestion or search results, the page doesn't exist
                raise PageError(title)

        return WikipediaPage(title, wiki=self, redirect=redirect, preload=preload)

    def search(self, query, results=10, suggestion=False):
        '''
        Do a Wikipedia search for `query`.

        Keyword arguments:

        * results - the maxmimum number of results returned
        * suggestion - if True, return results and suggestion (if any) in a tuple
        '''

        search_params = {
            'list': 'search',
            'srprop': 'snippet',
            'srlimit': results,
            'srsearch': query,
            'srsort': 'relevance'
        }
        if suggestion:
            search_params['srinfo'] = 'suggestion'

        raw_results = self.request(**search_params)
        pprint(raw_results)

        if 'error' in raw_results:
            if raw_results['error']['info'] in ('HTTP request timed out.', 'Pool queue is full'):
                raise HTTPTimeoutError(query)
            else:
                raise WikipediaException(raw_results['error']['info'])

        search_results = [
            (d['title'], bleach.clean(d['snippet'], strip=True))
            for d in raw_results['query']['search']
        ]

        if suggestion:
            if raw_results['query'].get('searchinfo'):
                return list(search_results), raw_results['query']['searchinfo']['suggestion']
            else:
                return list(search_results), None

        return search_results

    def geosearch(self, latitude, longitude, title=None, results=10, radius=1000):
        '''
        Do a wikipedia geo search for `latitude` and `longitude`
        using HTTP API described in http://www.mediawiki.org/wiki/Extension:GeoData

        Arguments:

        * latitude (float or decimal.Decimal)
        * longitude (float or decimal.Decimal)

        Keyword arguments:

        * title - The title of an article to search for
        * results - the maximum number of results returned
        * radius - Search radius in meters. The value must be between 10 and 10000
        '''

        search_params = {
            'list': 'geosearch',
            'gsradius': radius,
            'gscoord': f'{latitude}|{longitude}',
            'gslimit': results
        }
        if title:
            search_params['titles'] = title

        response = self.request(**search_params)

        if 'error' in response:
            if response['error']['info'] in ('HTTP request timed out.', 'Pool queue is full'):
                raise HTTPTimeoutError('{0}|{1}'.format(latitude, longitude))
            else:
                raise WikipediaException(response['error']['info'])

        search_pages = response['query'].get('pages', None)
        if search_pages:
            search_results = (v['title'] for k, v in search_pages.items() if k != '-1')
        else:
            search_results = (d['title'] for d in response['query']['geosearch'])

        return list(search_results)

    def suggest(self, query):
        '''
        Get a Wikipedia search suggestion for `query`.
        Returns a string or None if no suggestion was found.
        '''
        response = self.request(
            list='search',
            srinfo='suggestion',
            srprop='',
            srsearch=query
        )
        if response['query'].get('searchinfo'):
            return response['query']['searchinfo']['suggestion']

    def summary(self, title, sentences=0, chars=0, auto_suggest=True, redirect=True):
        '''
        Plain text summary of the page.

        .. note:: This is a convenience wrapper - auto_suggest and redirect are enabled by default

        Keyword arguments:

        * sentences - if set, return the first `sentences` sentences (can be no greater than 10).
        * chars - if set, return only the first `chars` characters (actual text returned may be slightly longer).
        * auto_suggest - let Wikipedia find a valid page title for the query
        * redirect - allow redirection without raising RedirectError
        '''

        # use auto_suggest and redirect to get the correct article
        # also, use page's error checking to raise DisambiguationError if necessary
        page_info = self.page(title, auto_suggest=auto_suggest, redirect=redirect)
        title = page_info.title
        pageid = page_info.pageid

        query_params = {'prop': 'extracts', 'explaintext': '', 'titles': title}

        if sentences:
            query_params['exsentences'] = sentences
        elif chars:
            query_params['exchars'] = chars
        else:
            query_params['exintro'] = ''

        request = self.request(**query_params)
        return request['query']['pages'][pageid]['extract']


def logger_config(level):
    logger.setLevel(level)
    handler = logging.StreamHandler()
    handler.setLevel(level)
    formatter = logging.Formatter('%(filename)s:%(funcName)s:%(lineno)s %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--pdb', action='store_true')
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument('-s', '--search', action='store_true')
    parser.add_argument('query', nargs='+')
    args = parser.parse_args()
    if args.pdb:
        try:
            import ipdb; ipdb.set_trace()
        except ImportError:
            import pdb; pdb.set_trace()

    logger_config(logging.DEBUG if args.verbose else logging.INFO)
    logger.debug(f'Got args {args}')
    query = ' '.join(args.query)
    wiki = Wikipedia()
    if args.search:
        listing, _ = wiki.search(query, suggestion=True)
        for title, snippet in listing:
            print(title)
            print(snippet, end='\n\n')
    else:
        pg = wiki.page(query, preload=True)
        pg.write()

    print()


if __name__ == '__main__':
    main()
